import argparse
import os
import pandas as pd
from pathlib import Path
from pydub import AudioSegment
import re
import sys
import xlsxwriter
from youtube_transcript_api import YouTubeTranscriptApi

"""
This script works together with youtube-dl to create datasets in the style of the ljspeech dataset for training
Tacotron 2.

Directions:
1. Create a list of YouTube urls of the videos you would like to make a dataset from and save them in a text file. One
    url per line.
2. Use youtube-dl to create .wav files from the YouTube videos and save them into a new directory.

To run this script you need to provide the following paths;
base_output_dir: The new base directory you created
url_for_vids: text document with urls to the YouTube videos you are using
"""

base_output_dir = '/Users/chenzhe/desktop/dt_audio_files/'
url_for_vids = '/Users/chenzhe/desktop/dt_audio_files/dt_audio_URLs_test'

# return a list of the video IDs from the text file with YouTube video urls
def strip_video_ids(path_to_URL_list):
    video_IDs = []
    urls_list = []
    with open(path_to_URL_list, 'r') as f:
        urls_list.append(list(f))
        for url in urls_list[0]:
            if url.find('=') != -1:
                video_IDs.append(url.split('=')[1].split('\n')[0])
        return video_IDs

# return a list of the transcripts generated by YouTubeTranscriptApi
def gen_transcripts(video_IDs_list):
    transcripts_list = []
    for video_id in video_IDs_list:
        transcripts_list.append(YouTubeTranscriptApi.get_transcript(video_id))
    return transcripts_list

# segments audio from videos based on the individual rows of subtitles from the YouTubeTranscriptApi and
# saves them to your output directory
# alpha determines what percentage of the estimated duration to include
def cut_wav(video_ID, transcript, base_output_dir, alpha):
    Path(base_output_dir + 'wavs/').mkdir(parents=True, exist_ok=True)
    output_dir = base_output_dir + 'wavs/'
    count = 1
    for i in range(len(transcript)):
        start_time = float(transcript[i]['start']) * 1000
        end_time = float(start_time) + (float(transcript[i]['duration']) * 1000 * alpha)
        newAudio = AudioSegment.from_wav(base_output_dir + video_ID + '.wav')
        newAudio = newAudio[start_time:end_time]
        newAudio.export(output_dir + video_ID + '-' + str(count).zfill(5) +'.wav', format='wav')
        count += 1

def iterate_wav_files(video_IDs_list, transcripts, base_output_dir, alpha=0.45):
    Path(base_output_dir + 'wavs/').mkdir(parents=True, exist_ok=True)
    output_dir = base_output_dir + 'wavs/'
    for i in range(len(video_IDs_list)):
        cut_wav(video_IDs_list[i], transcripts[i], base_output_dir, alpha=alpha)

# saves an excel workbook file from the transcript outputted by the YouTubeTranscriptApi module
def write_to_xlsx(workbooks_dir, transcript, video_ID):
    workbook = xlsxwriter.Workbook(workbooks_dir + video_ID + '.xlsx')
    worksheet = workbook.add_worksheet()
    cnt = 1
    xlsx_contents = []
    for i in range(len(transcript)):
        xlsx_contents.append([video_ID + '-' + str(cnt).zfill(5) + ' |', transcript[i]['text'].lower()])
        cnt += 1
    r, c = 0, 0
    for video, subtitles in xlsx_contents:
        worksheet.write(r, c, video)
        worksheet.write(r, c + 1, subtitles)
        r += 1
    workbook.close()

# iteratively calls the write_to_xlsx func to create excel workbook files for each of the videos
def make_xlsx_files(transcripts, video_IDs_list, base_output_dir):
    Path(base_output_dir + 'xlsx_workbooks/').mkdir(parents=True, exist_ok=True)
    workbooks_dir = base_output_dir + 'xlsx_workbooks/'
    cnt = 0
    for transcript in transcripts:
        write_to_xlsx(workbooks_dir, transcript, video_IDs_list[cnt])
        cnt += 1

# generates metadata filename
def make_metafile_name(base_output_dir):
    if 'metadata00.xlsx' not in os.listdir(base_output_dir):
        return base_output_dir + 'metadata00.xlsx'
    else:
        highest = 0
        for filename in os.listdir(base_output_dir):
            if 'metadata' in filename:
                filename = re.sub('[a-zA-Z.]', '', filename)
                highest = max(highest, int(filename))
        return 'metadata' + f'{highest + 1}'.zfill(2) + '.xlsx'

# combines all the excel workbooks into one file
def concat_xlsx_files(video_IDs_list, base_output_dir, del_temp_workbooks=True):
    metadata_filename = make_metafile_name(base_output_dir)
    Path(base_output_dir + 'xlsx_workbooks/').mkdir(parents=True, exist_ok=True)
    workbooks_dir = base_output_dir + 'xlsx_workbooks/'
    excel_filenames = list(map(lambda x: str(x) + '.xlsx', [x for x in video_IDs_list]))
    excels = [pd.read_excel(workbooks_dir + filename, header=None) for filename in excel_filenames]
    df = pd.concat(excels, ignore_index=True)
    df.to_excel(metadata_filename, header=None, index=False)
    if del_temp_workbooks:
        rm_temp_workbooks(video_IDs_list, base_output_dir)

def rm_temp_workbooks(video_IDs_list, base_output_dir):
    for i in range(len(video_IDs_list)):
        os.remove(base_output_dir + 'xlsx_workbooks/' + video_IDs_list[i] + '.xlsx')
    os.rmdir(base_output_dir + 'xlsx_workbooks/')

def rm_uncut_wav_files(video_IDs_list, base_output_dir):
    for i in range(len(video_IDs_list)):
        os.remove(base_output_dir + video_IDs_list[i] + '.wav')

parser = argparse.ArgumentParser(description='turn text file of YouTube urls and wav files from those videos in a dataset in the format of the Ljspeech dataset')
parser.add_argument('-urls', type=str, help='path of text file containing urls of YouTube videos')
parser.add_argument('-base_dir', type=str, help='path of base directory for output files')
args = parser.parse_args()

def main(url_for_vids, base_output_dir, del_uncut_wav_files=True):
    # arguments[0] = url_for_vids, get list of video IDs from text file of YouTube urls
    video_IDs_list = strip_video_ids(url_for_vids)

    # get list of transcripts from YouTube videos in video_IDs_list
    transcripts = gen_transcripts(video_IDs_list)

    # turn list of transcripts into xlsx files (excel workbooks) and save them as metadata_filename
    make_xlsx_files(transcripts, video_IDs_list, base_output_dir)
    concat_xlsx_files(video_IDs_list, base_output_dir)

    # cut wav files from youtube-dl
    iterate_wav_files(video_IDs_list, transcripts, base_output_dir, alpha=0.45)

    # delete uncut wav files to save flash mem
    if del_uncut_wav_files:
        rm_uncut_wav_files(video_IDs_list, base_output_dir)

if __name__ == '__main__':
    main(args.urls, args.base_dir)
